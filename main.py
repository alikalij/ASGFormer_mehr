# main.py

import torch
import torch.optim as optim
from torch_geometric.loader import DataLoader
from models.model import ASGFormer
from data.dataset import H5Dataset, PointCloudProcessor, read_file_list, compute_class_weights
from trainer import Trainer # âœ… ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©Ù„Ø§Ø³ Trainer Ø¬Ø¯ÛŒØ¯
from configs.env_config import CONFIG 
import os
import h5py

def main():
    # --- ØªÙ…Ø§Ù… ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„ Ú©Ø§Ù†ÙÛŒÚ¯ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ---
    hyperparams = CONFIG
    device = torch.device(hyperparams['device'])
    print(f"Using device: {device}")

    # --- Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø³Øª ---
    dataset_path = hyperparams['dataset_path']
    print(f"Loading dataset from: {dataset_path}")
    train_files = read_file_list(os.path.join(dataset_path, "list", "train5.txt"))
    val_files = read_file_list(os.path.join(dataset_path, "list", "val5.txt"))

    processor = PointCloudProcessor(num_points=hyperparams['num_points'])
    train_dataset = H5Dataset(train_files, processor, dataset_path)
    val_dataset = H5Dataset(val_files, processor, dataset_path)

    # ğŸ’¡ Ø¨Ù‡Ø¨ÙˆØ¯: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pin_memory Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ GPU
    train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'], shuffle=False, num_workers=4, pin_memory=True)

    # --- Ø³Ø§Ø®Øª Ù…Ø¯Ù„ ---
    print("Creating model...")
    model = ASGFormer(
        feature_dim=hyperparams['feature_dim'],
        main_input_dim=hyperparams['main_input_dim'],
        main_output_dim=hyperparams['num_classes'],
        stages_config=hyperparams['stages_config'], 
        knn_param=hyperparams['knn_param'],
        num_heads=hyperparams['num_heads'],
        neighbor_finder=hyperparams['neighbor_finder'], # âœ… Ø§Ø±Ø³Ø§Ù„
        search_radius=hyperparams['search_radius'],     # âœ… Ø§Ø±Ø³Ø§Ù„
        interpolation_k=hyperparams['interpolation_k'],
        dropout_param=hyperparams['dropout_param'],
        # âœ… Ø§Ø±Ø³Ø§Ù„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ KPConv Ø¨Ù‡ Ù…Ø¯Ù„
        kpconv_radius=hyperparams['kpconv_radius'],
        kpconv_kernel_size=hyperparams['kpconv_kernel_size']
        # ğŸ’¡ Ù†Ú©ØªÙ‡: Ø§Ø¨Ø¹Ø§Ø¯ Ø®Ø±ÙˆØ¬ÛŒ KPConv (64) Ùˆ ÙˆØ±ÙˆØ¯ÛŒ x_mlp (32)
        # Ø¯Ø± Ø¯Ø§Ø®Ù„ Ú©Ù„Ø§Ø³ ASGFormer Ù…Ø¯ÛŒØ±ÛŒØª Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø§Ø±Ø³Ø§Ù„ Ù†Ø¯Ø§Ø±Ù†Ø¯.
    )
    try:
        if int(torch.__version__.split('.')[0]) >= 2:
             print("Applying torch.compile...")
             #model = torch.compile(model)
        else:
             print("torch.compile requires PyTorch 2.0 or later.")
    except Exception as e:
        print(f"Warning: Failed to apply torch.compile: {e}")
        
    # --- ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ØŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ùˆ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ ---
    print("Setting up criterion, optimizer, and scheduler...")
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø±
    try:
        all_train_labels = [h5py.File(os.path.join(dataset_path, f), 'r')['label'][:] for f in train_files]
        class_weights = compute_class_weights(all_train_labels, num_classes=hyperparams['num_classes']).to(device)
        print("Class weights computed.")
    except Exception as e:
        print(f"Warning: Could not compute class weights: {e}. Using uniform weights.")
        class_weights = None # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ²Ù† ÛŒÚ©Ø³Ø§Ù† Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§

    criterion = torch.nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)
    optimizer = optim.AdamW(model.parameters(), lr=hyperparams['learning_rate'], weight_decay=hyperparams['weight_decay'])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=hyperparams['num_epochs'], eta_min=1e-6) # eta_min Ú©Ù…ÛŒ Ú©ÙˆÚ†Ú©ØªØ±

    # --- Ø³Ø§Ø®Øª Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Trainer ---
    print("Initializing Trainer...")
    trainer = Trainer(
        model=model, 
        train_loader=train_loader, 
        val_loader=val_loader, 
        criterion=criterion, 
        optimizer=optimizer, 
        scheduler=scheduler, 
        config=hyperparams
    )
    
    print("Starting training...")
    trainer.train()

if __name__ == "__main__":
    main()