# models/model.py

import torch
import torch.nn as nn
import math
from torch_geometric.nn import EdgeConv,MessagePassing, knn_graph, fps, knn
from torch_geometric.utils import softmax as pyg_softmax

class VirtualNode(nn.Module):
    """
    Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ú¯Ø±Ù‡ Ù…Ø¬Ø§Ø²ÛŒ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø·Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡ØŒ Ø¨Ø±Ø§ÛŒ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    Ùˆ ØªÙˆØ²ÛŒØ¹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¯Ø± Ú¯Ø±Ø§Ù Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯.
    Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ¹Ù„ÛŒ ÛŒÚ© Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡ Ùˆ Ù…Ø¤Ø«Ø± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø³Øª.
    """
    def __init__(self, hidden_dim):
        super().__init__()
        self.aggregate = nn.Linear(hidden_dim, hidden_dim)
        self.distribute = nn.Linear(hidden_dim, hidden_dim)
        self.norm = nn.LayerNorm(hidden_dim)

    def forward(self, x):
        if x.size(0) == 0:
            return x
        # Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¨Ø§ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ
        global_context = x.mean(dim=0, keepdim=True)
        global_context = self.aggregate(global_context)
        global_context = self.norm(global_context)
        # ØªÙˆØ²ÛŒØ¹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¨Ù‡ ØªÙ…Ø§Ù… Ú¯Ø±Ù‡â€ŒÙ‡Ø§
        return x + self.distribute(global_context)

class AdaptiveGraphTransformerBlock(MessagePassing):
    """
    Ø¨Ù„ÙˆÚ© Ø§ØµÙ„ÛŒ Adaptive Graph Transformer (AGT) Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ø¨Ø®Ø´ 3.2 Ù…Ù‚Ø§Ù„Ù‡.
    Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ²Ù†ÛŒ (Weighted Features) Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙØ§ÙˆØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯Ù‡ Ùˆ Ø§Ø² Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡ Ú¯Ø±Ø§ÙÛŒ (Graph Attention) Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    def __init__(self, in_channels, out_channels, dropout=0.1):
        super().__init__(aggr='add', flow='source_to_target')
        
        self.in_channels = in_channels
        self.out_channels = out_channels

        self.mlp_feature = nn.Sequential(
            nn.Linear(in_channels, out_channels),
            nn.ReLU(),
            nn.LayerNorm(out_channels)
        )
        # Ø§ÛŒÙ† MLP ÙˆÛŒÚ˜Ú¯ÛŒ ÙˆØ²Ù†ÛŒ W_ij Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙØ§ÙˆØª ÙˆÛŒÚ˜Ú¯ÛŒ (Î”f) Ùˆ ØªÙØ§ÙˆØª Ù…ÙˆÙ‚Ø¹ÛŒØª (Î”p) Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯
        self.mlp_weighted_feature = nn.Sequential(
            nn.Linear(out_channels + 3, out_channels),
            nn.ReLU(),
            nn.LayerNorm(out_channels) 
        )
        self.mlp_q = nn.Linear(out_channels, out_channels)
        self.mlp_k = nn.Linear(out_channels, out_channels)
        
        # Position Embedding Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙØ§ÙˆØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ù†Ø³Ø¨ÛŒ (Î”p_ij)
        self.pos_embedding = nn.Sequential(
            nn.Linear(3, out_channels),
            nn.ReLU(),
            nn.LayerNorm(out_channels)
        )
        self.final_norm = nn.LayerNorm(out_channels)
        self.dropout = nn.Dropout(p=dropout)

        # Ø§ØªØµØ§Ù„ Ú©ÙˆØªØ§Ù‡ (Residual Connection) Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø­Ùˆ Ø´Ø¯Ù† Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†
        if in_channels != out_channels:
            self.residual = nn.Linear(in_channels, out_channels)
        else:
            self.residual = nn.Identity()
            
    def forward(self, x, pos, edge_index):
        edge_index = edge_index.long()
        features = self.mlp_feature(x)
        
        # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ propagate Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø§Ø®Ù„ÛŒ message Ùˆ aggregate Ø±Ø§ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        updated_features = self.propagate(edge_index, x=features, pos=pos)
        
        # Ø§Ø¹Ù…Ø§Ù„ Ø§ØªØµØ§Ù„ Ú©ÙˆØªØ§Ù‡ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        output = self.final_norm(updated_features + self.residual(x))
        output = self.dropout(output)
        return output

    def message(self, x_i, x_j, pos_i, pos_j, index):
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Î”f Ùˆ Î”p Ø·Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡
        delta_f = x_i - x_j
        delta_p = pos_i - pos_j
        
        # Eq. (2) Ø¯Ø± Ù…Ù‚Ø§Ù„Ù‡: Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ ÙˆØ²Ù†ÛŒ W_ij
        concatenated_deltas = torch.cat([delta_f, delta_p], dim=-1)
        W_ij = self.mlp_weighted_feature(concatenated_deltas)
        
        # Eq. (4) Ø¯Ø± Ù…Ù‚Ø§Ù„Ù‡: Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡ Ú¯Ø±Ø§ÙÛŒ
        query_base = self.mlp_q(x_i)
        pos_emb = self.pos_embedding(delta_p)
        query = query_base + pos_emb
        key = self.mlp_k(W_ij)
        value = W_ij
        
        attention_score = (query * key).sum(dim=-1) / math.sqrt(self.out_channels)
        attention_weights = pyg_softmax(attention_score, index)
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± (value)
        return attention_weights.unsqueeze(-1) * value

class Stage(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, dropout_param=0.1):
        super(Stage, self).__init__()
        layers = []
        for i in range(num_layers):
            current_input_dim = input_dim if i == 0 else hidden_dim
            layers.append(AdaptiveGraphTransformerBlock(current_input_dim, hidden_dim, dropout_param))
        self.layers = nn.ModuleList(layers)

    def forward(self, x, pos, edge_index):
        if x.size(0) == 0:
            return x
        for layer in self.layers:
            x = layer(x, pos, edge_index)
        return x

class Encoder(nn.Module):
    def __init__(self, input_dim, stages_config, knn_param, dropout_param=0.1):
        super(Encoder, self).__init__()
        
        self.knn_param = knn_param
        self.stages = nn.ModuleList()
        self.virtual_nodes = nn.ModuleList()
        self.downsampling_ratios = []

        current_dim = input_dim
        for idx, stage_cfg in enumerate(stages_config):
            # Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ ÙÙ‚Ø· ÛŒÚ© MLP Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªØŒ Ø·Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡
            if idx == 0:
                self.stages.append(
                    nn.Sequential(
                        nn.Linear(current_dim, stage_cfg['hidden_dim']),
                        nn.ReLU(),
                        nn.LayerNorm(stage_cfg['hidden_dim']), # Ø¨Ù‡Ø¨ÙˆØ¯: Ø§ÙØ²ÙˆØ¯Ù† LayerNorm
                    )
                )
            else:
                self.stages.append(
                    Stage(input_dim=current_dim,
                          hidden_dim=stage_cfg['hidden_dim'],
                          num_layers=stage_cfg['num_layers'],
                          dropout_param=dropout_param)
                )

            self.virtual_nodes.append(VirtualNode(stage_cfg['hidden_dim']))
            self.downsampling_ratios.append(stage_cfg.get('downsample_ratio', None))
            current_dim = stage_cfg['hidden_dim']

    def forward(self, x, pos, labels, batch):
        features_list = [x]
        positions_list = [pos]
        labels_list = [labels]
        batches_list = [batch]

        current_x, current_pos, current_labels, current_batch = x, pos, labels, batch

        for stage_idx, (stage, virtual_node, ratio) in enumerate(zip(self.stages, self.virtual_nodes, self.downsampling_ratios)):
            if stage_idx > 0:
                current_x, current_pos, current_labels, current_batch = self._downsample(
                    current_x, current_pos, current_labels, current_batch, ratio
                )

            if current_x.size(0) == 0: break
            
            if isinstance(stage, nn.Sequential):
                current_x = stage(current_x)
            else:
                k_safe = min(self.knn_param, current_x.size(0) - 1)
                if k_safe > 0:
                    edge_index = knn_graph(current_pos, k=k_safe, batch=current_batch, loop=False)
                    current_x = stage(current_x, current_pos, edge_index)
            
            current_x = virtual_node(current_x)
            
            features_list.append(current_x)
            positions_list.append(current_pos)
            labels_list.append(current_labels)
            batches_list.append(current_batch)
            
        return features_list, positions_list, labels_list, batches_list
    
    def _downsample(self, x, pos, labels, batch, ratio):
        if ratio is None or ratio >= 1.0 or x.size(0) == 0:
            return x, pos, labels, batch
        
        mask = fps(pos, batch, ratio=ratio)
        return x[mask], pos[mask], labels[mask], batch[mask] if batch is not None else None

# Ø¨Ù‡Ø¨ÙˆØ¯: Ø±ÙØ¹ Ú©Ø§Ù…Ù„ Ø®Ø·Ø§ÛŒ CUDA Ø¨Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø³ØªÛŒ Ø¨Ø§ knn_interpolate
class InterpolationStage(nn.Module):
    def __init__(self, coarse_dim, fine_dim, out_dim, knn_param, dropout_param=0.1):
        super(InterpolationStage, self).__init__()
        
        self.knn_param = knn_param
        self.mlp = nn.Sequential(
            nn.Linear(coarse_dim + fine_dim, out_dim),
            nn.ReLU(),
            nn.LayerNorm(out_dim),
            nn.Dropout(p=dropout_param)
        )

    def forward(self, coarse_features, coarse_pos, fine_features, fine_pos):
        if coarse_pos.size(0) == 0:
            # Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ù„Ø§ÛŒÙ‡ Ø¯Ø±Ø´Øª Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¯Ø±ÙˆÙ†â€ŒÛŒØ§Ø¨ÛŒ Ú©Ø±Ø¯.
            # Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„ØªØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø±ÛŒØ² Ø±Ø§ Ø¨Ø§ ÛŒÚ© MLP Ø³Ø§Ø¯Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            return self.mlp(torch.cat([torch.zeros_like(fine_features), fine_features], dim=1))

        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² knn_interpolate Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÙˆÙ†â€ŒÛŒØ§Ø¨ÛŒ Ø§Ù…Ù† Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡
        # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø¯Ø± fine_posØŒ Ø³Ù‡ Ù‡Ù…Ø³Ø§ÛŒÙ‡ Ù†Ø²Ø¯ÛŒÚ© Ø¯Ø± coarse_pos Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ§ØµÙ„Ù‡ ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ù‡ Ù†Ù‚Ø·Ù‡ fine_pos Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        k_safe = min(self.knn_param, coarse_pos.size(0))
        interpolated_features = knn_interpolate(
            coarse_features, coarse_pos, fine_pos, k=k_safe
        )

        # ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÙˆÙ†â€ŒÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø§Ø² Ø·Ø±ÛŒÙ‚ skip-connection
        combined = torch.cat([interpolated_features, fine_features], dim=1)
        return self.mlp(combined)

class Decoder(nn.Module):
    def __init__(self, main_output_dim, stages_config, knn_param, dropout_param=0.1):
        super(Decoder, self).__init__()
        
        self.stages = nn.ModuleList()
        num_encoder_stages = len(stages_config)

        # Ø¯ÛŒÚ©ÙˆØ¯Ø± ÛŒÚ© Ù…Ø±Ø­Ù„Ù‡ Ú©Ù…ØªØ± Ø§Ø² Ø§Ù†Ú©ÙˆØ¯Ø± Ø¯Ø§Ø±Ø¯
        for i in range(num_encoder_stages - 1):
            coarse_stage_cfg = stages_config[-(i + 1)] 
            fine_stage_cfg = stages_config[-(i + 2)]
            
            self.stages.append(
                InterpolationStage(
                    coarse_dim=coarse_stage_cfg['hidden_dim'],
                    fine_dim=fine_stage_cfg['hidden_dim'],
                    out_dim=fine_stage_cfg['hidden_dim'],
                    knn_param=knn_param,
                    dropout_param=dropout_param
                )
            )
            
        self.final_mlp = nn.Sequential(
            nn.Linear(stages_config[0]['hidden_dim'], stages_config[0]['hidden_dim']),
            nn.ReLU(),
            nn.Linear(stages_config[0]['hidden_dim'], main_output_dim)
        )

    def forward(self, encoder_features, positions, sampled_labels, batches):
        if not encoder_features:
            return torch.empty(0), torch.empty(0)

        # Ø´Ø±ÙˆØ¹ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ø®Ø±ÛŒÙ† (Ø¯Ø±Ø´Øªâ€ŒØªØ±ÛŒÙ†) Ù„Ø§ÛŒÙ‡ Ø§Ù†Ú©ÙˆØ¯Ø±
        x = encoder_features.pop()
        pos = positions.pop()

        for stage in self.stages:
            # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø±ÛŒØ²ØªØ± (Ø§ØªØµØ§Ù„ Ú©ÙˆØªØ§Ù‡)
            skip_features = encoder_features.pop()
            skip_pos = positions.pop()
            
            x = stage(x, pos, skip_features, skip_pos)
            pos = skip_pos # Ù…ÙˆÙ‚Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù„Ø§ÛŒÙ‡ Ø±ÛŒØ²ØªØ± Ø¨Ù‡â€ŒØ±ÙˆØ² Ù…ÛŒâ€ŒØ´ÙˆØ¯

        final_labels = sampled_labels[0]
        return self.final_mlp(x), final_labels

class ASGFormer(nn.Module):
    def __init__(self, feature_dim, main_input_dim, main_output_dim, stages_config, knn_param, dropout_param=0.1, kpconv_radius=0.1, kpconv_kernel_size=15):
        """
        Args:
            kpconv_radius (float): Ø´Ø¹Ø§Ø¹ Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ KPConv Ø§ÙˆÙ„ÛŒÙ‡.
            kpconv_kernel_size (int): ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ø±Ù†Ù„ Ø¨Ø±Ø§ÛŒ KPConv Ø§ÙˆÙ„ÛŒÙ‡.
        """
        super(ASGFormer, self).__init__()
        
        # --- Û±. Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ÙˆÙ„ÛŒÙ‡ EdgeConv (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† KPConv) ---
        edgeconv_output_dim = 64 # Ø§Ø¨Ø¹Ø§Ø¯ Ø®Ø±ÙˆØ¬ÛŒ Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ÙˆÙ„ÛŒÙ‡
        
        # EdgeConv ÛŒÚ© MLP Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ ØªØ§ Ø±ÙˆÛŒ ÛŒØ§Ù„â€ŒÙ‡Ø§ Ø§Ø¹Ù…Ø§Ù„ Ø´ÙˆØ¯
        # ÙˆØ±ÙˆØ¯ÛŒ MLP: (2 * feature_dim) -> (ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ù‚Ø·Ù‡ Ù…Ø±Ú©Ø²ÛŒ + ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ù…Ø³Ø§ÛŒÙ‡)
        # ğŸ’¡ Ù†Ú©ØªÙ‡: Ù…Ø§ Ø§Ø² (2 * (feature_dim + 3)) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ pos Ø±Ø§ Ù‡Ù… ØµØ±ÛŒØ­Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒÙ…
        # Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ù‡ EdgeConv Ù‚Ø¯Ø±Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ø¯Ø±Ú© Ù‡Ù†Ø¯Ø³Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
        
        # Ù…Ø§ Ø§Ø² ÛŒÚ© MLP Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ EdgeConv Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:
        initial_encoder_nn = nn.Sequential(
            nn.Linear(2 * (feature_dim + 3), edgeconv_output_dim), # (2 * (9+3)) = 24
            nn.ReLU(),
            nn.LayerNorm(edgeconv_output_dim)
        )

        print(f"Initializing EdgeConv layer with input MLP: 2*({feature_dim}+3) -> {edgeconv_output_dim}")
        # âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„Ø§ÛŒÙ‡ EdgeConv Ú©Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒÙ… Ø¯Ø± Ù…Ø­ÛŒØ· Ø´Ù…Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
        self.initial_encoder_conv = EdgeConv(nn=initial_encoder_nn, aggr='max')
        self.initial_encoder_norm = nn.LayerNorm(edgeconv_output_dim)
        
        # --- Û±. Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ÙˆÙ„ÛŒÙ‡ KPConv ---
        # Ø§Ø¨Ø¹Ø§Ø¯ Ø®Ø±ÙˆØ¬ÛŒ Ø§ÛŒÙ† Ù„Ø§ÛŒÙ‡ (kpconv_output_dim) ÛŒÚ© Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø¬Ø¯ÛŒØ¯ Ø§Ø³Øª
        kpconv_output_dim = 64
        print(f"Initializing KPConv layer with in_channels={feature_dim}, out_channels={kpconv_output_dim}, radius={kpconv_radius}")
        self.initial_kpconv = KPConv(
            in_channels=feature_dim,        # ÙˆØ±ÙˆØ¯ÛŒ: 9 ÙˆÛŒÚ˜Ú¯ÛŒ Ø®Ø§Ù…
            out_channels=kpconv_output_dim, # Ø®Ø±ÙˆØ¬ÛŒ: 64 ÙˆÛŒÚ˜Ú¯ÛŒ ØºÙ†ÛŒâ€ŒØ´Ø¯Ù‡ Ù…Ø­Ù„ÛŒ
            dim=3,
            kernel_size=kpconv_kernel_size,
            radius=kpconv_radius,
            aggr='mean' # ÛŒØ§ aggr='add'
        )
        # ğŸ’¡ Ù†Ú©ØªÙ‡: Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø®ÙˆØ§Ù‡ÛŒØ¯ ÛŒÚ© LayerNorm ÛŒØ§ ReLU Ø¨Ø¹Ø¯ Ø§Ø² KPConv Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
        self.kpconv_norm = nn.LayerNorm(kpconv_output_dim)

        # --- Û². MLPÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ÑĞ¼Ø¨Ø¯ÛŒÙ†Ú¯ ---
        print(f"Initializing Embedding MLPs: x_mlp input={edgeconv_output_dim}, pos_mlp input=3, output={main_input_dim}")
        # âœ… ÙˆØ±ÙˆØ¯ÛŒ x_mlp Ø§Ú©Ù†ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ KPConv Ø§Ø³Øª (kpconv_output_dim)
        self.x_mlp = nn.Sequential(
            nn.Linear(edgeconv_output_dim, main_input_dim),
            nn.ReLU(),
            nn.LayerNorm(main_input_dim)
        )        
        
        self.pos_mlp = nn.Sequential(
            nn.Linear(3, main_input_dim),
            nn.ReLU(),
            nn.LayerNorm(main_input_dim)
        )

        # --- Û³. Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ØµÙ„ÛŒ ASGFormer ---
        print(f"Initializing Main Encoder with input_dim={main_input_dim}")
        self.encoder = Encoder(
            input_dim=main_input_dim, # ÙˆØ±ÙˆØ¯ÛŒ Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ØµÙ„ÛŒ main_input_dim Ø§Ø³Øª
            stages_config=stages_config,
            knn_param=knn_param,
            dropout_param=dropout_param
        )

        # --- Û´. Ø¯ÛŒÚ©ÙˆØ¯Ø± Ø§ØµÙ„ÛŒ ASGFormer ---
        print("Initializing Main Decoder...")
        self.decoder = Decoder(
            main_output_dim=main_output_dim,
            stages_config=stages_config,
            knn_param=knn_param,
            dropout_param=dropout_param
        )

        # --- Ûµ. Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ ---
        self._initialize_weights()
        print("Model Initialization Complete.")
        

    def forward(self, data):
        # âœ… Ø§ØµÙ„Ø§Ø­: ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„ Ø§Ú©Ù†ÙˆÙ† Ø¢Ø¨Ø¬Ú©Øª data Ø§Ø² PyG Ø§Ø³Øª
        x_initial, pos, labels, batch = data.x, data.pos, data.y, data.batch
        # x_initial: [N, 9], pos: [N, 3], labels: [N], batch: [N]

        # --- Û±. Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ÙˆÙ„ÛŒÙ‡ EdgeConv ---        
        # ğŸ’¡ EdgeConv Ø¨Ù‡ ÛŒÚ© Ú¯Ø±Ø§Ù Ù‡Ù…Ø³Ø§ÛŒÚ¯ÛŒ (edge_index) Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯
        # Ù…Ø§ Ø¢Ù† Ø±Ø§ Ø¨Ø§ knn_graph (Ú©Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒÙ… Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯) Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ…
        # Ø§Ø² Ù‡Ù…Ø§Ù† k_param Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ù‡Ø±Ù‡ Ù…ÛŒâ€ŒØ¨Ø±ÛŒÙ….
        k = self.encoder.knn_param # Ú¯Ø±ÙØªÙ† k Ø§Ø² Ø§Ù†Ú©ÙˆØ¯Ø± (e.g., 16)
        k_safe = min(k, x_initial.size(0) - 1)
        if k_safe <= 0: k_safe = 1 # Ø­Ø¯Ø§Ù‚Ù„ 1 Ù‡Ù…Ø³Ø§ÛŒÙ‡

        edge_index = knn_graph(pos, k=k_safe, batch=batch, loop=False)

        # ğŸ’¡ ØªØ±Ú©ÛŒØ¨ X Ùˆ Pos Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ ØºÙ†ÛŒâ€ŒØªØ± Ø¨Ù‡ EdgeConv
        # Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ù‡ MLP Ø¯Ø§Ø®Ù„ EdgeConv Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ù‡Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù‡Ù… Ù…ÙˆÙ‚Ø¹ÛŒØª Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ø¯
        combined_x_pos = torch.cat([x_initial, pos], dim=-1) # [N, 12]

        # Ø§Ø¬Ø±Ø§ÛŒ EdgeConv
        # ÙˆØ±ÙˆØ¯ÛŒ: (x, edge_index) -> (ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ØŒ Ú¯Ø±Ø§Ù)
        x_encoded = self.initial_encoder_conv(x=combined_x_pos, edge_index=edge_index)
        x_encoded = self.initial_encoder_norm(x_encoded) # [N, 64]

        # --- Û±. Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ÙˆÙ„ÛŒÙ‡ KPConv ---
        # KPConv ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ù„ÛŒ ØºÙ†ÛŒâ€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        # ÙˆØ±ÙˆØ¯ÛŒ: x_initial (9 Ø¨Ø¹Ø¯ÛŒ), pos, batch
        # Ø®Ø±ÙˆØ¬ÛŒ: x_encoded (64 Ø¨Ø¹Ø¯ÛŒ)
        # print(f"KPConv Input shapes: x={x_initial.shape}, pos={pos.shape}, batch={batch.shape if batch is not None else 'None'}")
        x_encoded2 = self.initial_kpconv(x=x_initial, pos=pos, batch=batch)
        x_encoded2 = self.kpconv_norm(x_encoded2) # Ø§Ø¹Ù…Ø§Ù„ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        # print(f"KPConv Output shape: {x_encoded.shape}") # Should be [N, 64]

        # --- Û². Ø§Ø¬Ø±Ø§ÛŒ MLPÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ÑĞ¼Ø¨Ø¯ÛŒÙ†Ú¯ ---
        # âœ… x_mlp Ø§Ú©Ù†ÙˆÙ† Ø±ÙˆÛŒ Ø®Ø±ÙˆØ¬ÛŒ KPConv Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        x_emb = self.x_mlp(x_encoded) # ÙˆØ±ÙˆØ¯ÛŒ: [N, 64], Ø®Ø±ÙˆØ¬ÛŒ: [N, main_input_dim=32]
        # pos_mlp Ù‡Ù…Ú†Ù†Ø§Ù† Ø±ÙˆÛŒ pos Ø§ØµÙ„ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        pos_emb = self.pos_mlp(pos)   # ÙˆØ±ÙˆØ¯ÛŒ: [N, 3], Ø®Ø±ÙˆØ¬ÛŒ: [N, main_input_dim=32]
        # print(f"Embedding shapes: x_emb={x_emb.shape}, pos_emb={pos_emb.shape}")
        
        # ØªØ±Ú©ÛŒØ¨ Ø¯Ùˆ ÑĞ¼Ø¨Ø¯ÛŒÙ†Ú¯
        combined_features = x_emb + pos_emb # [N, main_input_dim=32]
        # print(f"Combined features shape: {combined_features.shape}")
        
        # --- Û³. Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù†Ú©ÙˆØ¯Ø± Ùˆ Ø¯ÛŒÚ©ÙˆØ¯Ø± Ø§ØµÙ„ÛŒ ASGFormer ---
        # Ø§Ù†Ú©ÙˆØ¯Ø± Ø§ØµÙ„ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØ±Ú©ÛŒØ¨â€ŒØ´Ø¯Ù‡ Ùˆ pos Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        # print("Entering Main Encoder...")
        encoder_features, positions, sampled_labels, batches = self.encoder(combined_features, pos, labels, batch)
        # print("Exited Main Encoder. Entering Main Decoder...")

        # ğŸ’¡ Ù†Ú©ØªÙ‡: Ø§Ú¯Ø± KPConv Ø¯Ø§ÙˆÙ†â€ŒØ³Ù…Ù¾Ù„ÛŒÙ†Ú¯ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ø§Ø¯ØŒ Ø¨Ø§ÛŒØ¯ skip connections Ø¯ÛŒÚ©ÙˆØ¯Ø± Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ø±Ø¯ÛŒÙ….
        # Ø§Ù…Ø§ KPConv Ø³Ø§Ø¯Ù‡ØŒ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ø±Ø§ ØªØºÛŒÛŒØ± Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

        logits, final_labels_from_decoder = self.decoder(encoder_features, positions, sampled_labels, batches)
        # print("Exited Main Decoder.")
        # print(f"Logits shape: {logits.shape}, Final Labels shape: {final_labels_from_decoder.shape}")

        # âœ… Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ labels Ø§ØµÙ„ÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù†Ù‡ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø¯Ø§ÙˆÙ†â€ŒØ³Ù…Ù¾Ù„ Ø´Ø¯Ù‡
        return logits, labels
            
        
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            # ğŸ’¡ Ø¨Ù‡Ø¨ÙˆØ¯: Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø®Ø§ØµÛŒ Ø¨Ø±Ø§ÛŒ KPConv Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
            elif isinstance(m, KPConv):
                 nn.init.xavier_uniform_(m.weight)
                 if m.bias is not None:
                     nn.init.zeros_(m.bias)