import torch
import importlib.util
import os
import sys
import torch.nn as nn
import torch.nn.functional as F
import math
import torch_geometric.nn as pyg_nn
from torch_geometric.utils import scatter as pyg_scatter
import warnings
from torch_geometric.nn import knn_graph, knn_interpolate, fps
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import softmax as pyg_softmax


class VirtualNode(nn.Module):
    """
    Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ú¯Ø±Ù‡ Ù…Ø¬Ø§Ø²ÛŒ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø·Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡ØŒ Ø¨Ø±Ø§ÛŒ Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
    Ùˆ ØªÙˆØ²ÛŒØ¹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¯Ø± Ú¯Ø±Ø§Ù Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯.
    Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ¹Ù„ÛŒ ÛŒÚ© Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡ Ùˆ Ù…Ø¤Ø«Ø± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø³Øª.
    """
    def __init__(self, hidden_dim):
        super().__init__()
        self.aggregate = nn.Linear(hidden_dim, hidden_dim)
        self.distribute = nn.Linear(hidden_dim, hidden_dim)
        self.norm = nn.LayerNorm(hidden_dim)

    def forward(self, x):
        if x.size(0) == 0:
            return x
        # Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¨Ø§ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ
        global_context = x.mean(dim=0, keepdim=True)
        global_context = self.aggregate(global_context)
        global_context = self.norm(global_context)
        # ØªÙˆØ²ÛŒØ¹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¨Ù‡ ØªÙ…Ø§Ù… Ú¯Ø±Ù‡â€ŒÙ‡Ø§
        return x + self.distribute(global_context)

class AdaptiveGraphTransformerBlock(MessagePassing):
    """
    Ø¨Ù„ÙˆÚ© Ø§ØµÙ„ÛŒ Adaptive Graph Transformer (AGT) Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ø¨Ø®Ø´ 3.2 Ù…Ù‚Ø§Ù„Ù‡.
    Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ²Ù†ÛŒ (Weighted Features) Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙØ§ÙˆØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯Ù‡ Ùˆ Ø§Ø² Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡ Ú¯Ø±Ø§ÙÛŒ (Graph Attention) Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    def __init__(self, in_channels, out_channels, dropout=0.1):
        super().__init__(aggr='add', flow='source_to_target')
        
        self.in_channels = in_channels
        self.out_channels = out_channels

        self.mlp_feature = nn.Sequential(
            nn.Linear(in_channels, out_channels),
            nn.ReLU(),
            nn.LayerNorm(out_channels)
        )
        # Ø§ÛŒÙ† MLP ÙˆÛŒÚ˜Ú¯ÛŒ ÙˆØ²Ù†ÛŒ W_ij Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙØ§ÙˆØª ÙˆÛŒÚ˜Ú¯ÛŒ (Î”f) Ùˆ ØªÙØ§ÙˆØª Ù…ÙˆÙ‚Ø¹ÛŒØª (Î”p) Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯
        self.mlp_weighted_feature = nn.Sequential(
            nn.Linear(out_channels + 3, out_channels),
            nn.ReLU(),
            nn.LayerNorm(out_channels) 
        )
        self.mlp_q = nn.Linear(out_channels, out_channels)
        self.mlp_k = nn.Linear(out_channels, out_channels)
        
        # Position Embedding Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙØ§ÙˆØª Ù…ÙˆÙ‚Ø¹ÛŒØª Ù†Ø³Ø¨ÛŒ (Î”p_ij)
        self.pos_embedding = nn.Sequential(
            nn.Linear(3, out_channels),
            nn.ReLU(),
            nn.LayerNorm(out_channels)
        )
        self.final_norm = nn.LayerNorm(out_channels)
        self.dropout = nn.Dropout(p=dropout)

        # Ø§ØªØµØ§Ù„ Ú©ÙˆØªØ§Ù‡ (Residual Connection) Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø­Ùˆ Ø´Ø¯Ù† Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†
        if in_channels != out_channels:
            self.residual = nn.Linear(in_channels, out_channels)
        else:
            self.residual = nn.Identity()
            
    def forward(self, x, pos, edge_index):
        edge_index = edge_index.long()
        features = self.mlp_feature(x)
        
        # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ propagate Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø§Ø®Ù„ÛŒ message Ùˆ aggregate Ø±Ø§ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        updated_features = self.propagate(edge_index, x=features, pos=pos)
        
        # Ø§Ø¹Ù…Ø§Ù„ Ø§ØªØµØ§Ù„ Ú©ÙˆØªØ§Ù‡ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        output = self.final_norm(updated_features + self.residual(x))
        output = self.dropout(output)
        return output

    def message(self, x_i, x_j, pos_i, pos_j, index):
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Î”f Ùˆ Î”p Ø·Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡
        delta_f = x_i - x_j
        delta_p = pos_i - pos_j
        
        # Eq. (2) Ø¯Ø± Ù…Ù‚Ø§Ù„Ù‡: Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ ÙˆØ²Ù†ÛŒ W_ij
        concatenated_deltas = torch.cat([delta_f, delta_p], dim=-1)
        W_ij = self.mlp_weighted_feature(concatenated_deltas)
        
        # Eq. (4) Ø¯Ø± Ù…Ù‚Ø§Ù„Ù‡: Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡ Ú¯Ø±Ø§ÙÛŒ
        query_base = self.mlp_q(x_i)
        pos_emb = self.pos_embedding(delta_p)
        query = query_base + pos_emb
        key = self.mlp_k(W_ij)
        value = W_ij
        
        attention_score = (query * key).sum(dim=-1) / math.sqrt(self.out_channels)
        attention_weights = pyg_softmax(attention_score, index)
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± (value)
        return attention_weights.unsqueeze(-1) * value

class Stage(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, dropout_param=0.1):
        super(Stage, self).__init__()
        layers = []
        for i in range(num_layers):
            current_input_dim = input_dim if i == 0 else hidden_dim
            layers.append(AdaptiveGraphTransformerBlock(current_input_dim, hidden_dim, dropout_param))
        self.layers = nn.ModuleList(layers)

    def forward(self, x, pos, edge_index):
        if x.size(0) == 0:
            return x
        for layer in self.layers:
            x = layer(x, pos, edge_index)
        return x

class Encoder(nn.Module):
    def __init__(self, input_dim, stages_config, knn_param, dropout_param=0.1):
        super(Encoder, self).__init__()
        
        self.knn_param = knn_param
        self.stages = nn.ModuleList()
        self.virtual_nodes = nn.ModuleList()
        self.downsampling_ratios = []

        current_dim = input_dim
        for idx, stage_cfg in enumerate(stages_config):
            # Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ ÙÙ‚Ø· ÛŒÚ© MLP Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªØŒ Ø·Ø¨Ù‚ Ù…Ù‚Ø§Ù„Ù‡
            if idx == 0:
                self.stages.append(
                    nn.Sequential(
                        nn.Linear(current_dim, stage_cfg['hidden_dim']),
                        nn.ReLU(),
                        nn.LayerNorm(stage_cfg['hidden_dim']), # Ø¨Ù‡Ø¨ÙˆØ¯: Ø§ÙØ²ÙˆØ¯Ù† LayerNorm
                    )
                )
            else:
                self.stages.append(
                    Stage(input_dim=current_dim,
                          hidden_dim=stage_cfg['hidden_dim'],
                          num_layers=stage_cfg['num_layers'],
                          dropout_param=dropout_param)
                )

            self.virtual_nodes.append(VirtualNode(stage_cfg['hidden_dim']))
            self.downsampling_ratios.append(stage_cfg.get('downsample_ratio', None))
            current_dim = stage_cfg['hidden_dim']

    def forward(self, x, pos, labels):
        features = [x]
        positions = [pos]
        sampled_labels = [labels]

        for stage_idx, (stage, virtual_node, ratio) in enumerate(zip(self.stages, self.virtual_nodes, self.downsampling_ratios)):
            
            # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ú©Ø§Ù‡Ø´ÛŒ (Downsampling) Ø¨Ø±Ø§ÛŒ Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ Ø§Ø² Ø§ÙˆÙ„
            if stage_idx > 0:
                x, pos, labels = self._downsample(x, pos, labels, ratio)

            if x.size(0) == 0:
                print(f"ðŸ›‘ Ù‡Ø´Ø¯Ø§Ø±: Ø§Ø¬Ø±Ø§ÛŒ Encoder Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ ØµÙØ± Ø´Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ø¯Ø± Stage {stage_idx} Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
                break
            
            if isinstance(stage, nn.Sequential):
                x = stage(x)
            else:
                # Ø¨Ù‡Ø¨ÙˆØ¯: k Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø§ÛŒÙ…Ù† ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ø¨ÛŒØ´ØªØ± Ù†Ø¨Ø§Ø´Ø¯
                k_safe = min(self.knn_param, x.size(0) -1) # k Ø¨Ø§ÛŒØ¯ Ø§Ø² N Ú©ÙˆÚ†Ú©ØªØ± Ø¨Ø§Ø´Ø¯
                if k_safe > 0:
                    edge_index = knn_graph(pos, k=k_safe, loop=False)
                    x = stage(x, pos, edge_index)
            
            x = virtual_node(x)
            
            features.append(x)
            positions.append(pos)
            sampled_labels.append(labels)
            
        return features, positions, sampled_labels

    def _downsample(self, x, pos, labels, ratio):
        num_points_to_keep = int(x.size(0) * ratio)
        if num_points_to_keep < 1:
            return torch.empty(0, x.size(1), device=x.device), \
                   torch.empty(0, 3, device=pos.device), \
                   torch.empty(0, device=labels.device)
        
        # Ø¨Ù‡Ø¨ÙˆØ¯: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ fps Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø² PyG
        mask = fps(pos, ratio=ratio)
        return x[mask], pos[mask], labels[mask]

# Ø¨Ù‡Ø¨ÙˆØ¯: Ø±ÙØ¹ Ú©Ø§Ù…Ù„ Ø®Ø·Ø§ÛŒ CUDA Ø¨Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø³ØªÛŒ Ø¨Ø§ knn_interpolate
class InterpolationStage(nn.Module):
    def __init__(self, coarse_dim, fine_dim, out_dim, knn_param, dropout_param=0.1):
        super(InterpolationStage, self).__init__()
        
        self.knn_param = knn_param
        self.mlp = nn.Sequential(
            nn.Linear(coarse_dim + fine_dim, out_dim),
            nn.ReLU(),
            nn.LayerNorm(out_dim),
            nn.Dropout(p=dropout_param)
        )

    def forward(self, coarse_features, coarse_pos, fine_features, fine_pos):
        if coarse_pos.size(0) == 0:
            # Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ù„Ø§ÛŒÙ‡ Ø¯Ø±Ø´Øª Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¯Ø±ÙˆÙ†â€ŒÛŒØ§Ø¨ÛŒ Ú©Ø±Ø¯.
            # Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„ØªØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø±ÛŒØ² Ø±Ø§ Ø¨Ø§ ÛŒÚ© MLP Ø³Ø§Ø¯Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            return self.mlp(torch.cat([torch.zeros_like(fine_features), fine_features], dim=1))

        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² knn_interpolate Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÙˆÙ†â€ŒÛŒØ§Ø¨ÛŒ Ø§Ù…Ù† Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡
        # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø¯Ø± fine_posØŒ Ø³Ù‡ Ù‡Ù…Ø³Ø§ÛŒÙ‡ Ù†Ø²Ø¯ÛŒÚ© Ø¯Ø± coarse_pos Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ§ØµÙ„Ù‡ ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ù‡ Ù†Ù‚Ø·Ù‡ fine_pos Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        k_safe = min(self.knn_param, coarse_pos.size(0))
        interpolated_features = knn_interpolate(
            coarse_features, coarse_pos, fine_pos, k=k_safe
        )

        # ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÙˆÙ†â€ŒÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø§Ø² Ø·Ø±ÛŒÙ‚ skip-connection
        combined = torch.cat([interpolated_features, fine_features], dim=1)
        return self.mlp(combined)

class Decoder(nn.Module):
    def __init__(self, main_output_dim, stages_config, knn_param, dropout_param=0.1):
        super(Decoder, self).__init__()
        
        self.stages = nn.ModuleList()
        num_encoder_stages = len(stages_config)

        # Ø¯ÛŒÚ©ÙˆØ¯Ø± ÛŒÚ© Ù…Ø±Ø­Ù„Ù‡ Ú©Ù…ØªØ± Ø§Ø² Ø§Ù†Ú©ÙˆØ¯Ø± Ø¯Ø§Ø±Ø¯
        for i in range(num_encoder_stages - 1):
            coarse_stage_cfg = stages_config[-(i + 1)] 
            fine_stage_cfg = stages_config[-(i + 2)]
            
            self.stages.append(
                InterpolationStage(
                    coarse_dim=coarse_stage_cfg['hidden_dim'],
                    fine_dim=fine_stage_cfg['hidden_dim'],
                    out_dim=fine_stage_cfg['hidden_dim'],
                    knn_param=knn_param,
                    dropout_param=dropout_param
                )
            )
            
        self.final_mlp = nn.Sequential(
            nn.Linear(stages_config[0]['hidden_dim'], stages_config[0]['hidden_dim']),
            nn.ReLU(),
            nn.Linear(stages_config[0]['hidden_dim'], main_output_dim)
        )

    def forward(self, encoder_features, positions, sampled_labels):
        if not encoder_features:
            return torch.empty(0), torch.empty(0)

        # Ø´Ø±ÙˆØ¹ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ø®Ø±ÛŒÙ† (Ø¯Ø±Ø´Øªâ€ŒØªØ±ÛŒÙ†) Ù„Ø§ÛŒÙ‡ Ø§Ù†Ú©ÙˆØ¯Ø±
        x = encoder_features.pop()
        pos = positions.pop()

        for stage in self.stages:
            # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø±ÛŒØ²ØªØ± (Ø§ØªØµØ§Ù„ Ú©ÙˆØªØ§Ù‡)
            skip_features = encoder_features.pop()
            skip_pos = positions.pop()
            
            x = stage(x, pos, skip_features, skip_pos)
            pos = skip_pos # Ù…ÙˆÙ‚Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù„Ø§ÛŒÙ‡ Ø±ÛŒØ²ØªØ± Ø¨Ù‡â€ŒØ±ÙˆØ² Ù…ÛŒâ€ŒØ´ÙˆØ¯

        final_labels = sampled_labels[0]
        return self.final_mlp(x), final_labels

class ASGFormer(nn.Module):
    def __init__(self, feature_dim, main_input_dim, main_output_dim, stages_config, knn_param, dropout_param=0.1):
        super(ASGFormer, self).__init__()
        
        self.x_mlp = nn.Sequential(
            nn.Linear(feature_dim, main_input_dim),
            nn.ReLU(),
            nn.LayerNorm(main_input_dim)
        )
        self.pos_mlp = nn.Sequential(
            nn.Linear(3, main_input_dim),
            nn.ReLU(),
            nn.LayerNorm(main_input_dim)
        )
        self.encoder = Encoder(
            input_dim=main_input_dim,
            stages_config=stages_config,
            knn_param=knn_param,
            dropout_param=dropout_param
        )
        self.decoder = Decoder(
            main_output_dim=main_output_dim,
            stages_config=stages_config,
            knn_param=knn_param,
            dropout_param=dropout_param
        )
        self._initialize_weights()

    def forward(self, x, pos, labels):
        x_emb = self.x_mlp(x)
        pos_emb = self.pos_mlp(pos)
        combined_features = x_emb + pos_emb 
        
        # Ø¯Ø± Ø§Ù†Ú©ÙˆØ¯Ø±ØŒ Ù…Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ (Ù‚Ø¨Ù„ Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† MLP) Ø±Ø§ Ù‡Ù… Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        # Ú†ÙˆÙ† Ø¯ÛŒÚ©ÙˆØ¯Ø± Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ú©ÙˆØªØ§Ù‡ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯
        initial_features = combined_features
        initial_pos = pos
        initial_labels = labels
        
        encoder_features_list, positions_list, sampled_labels_list = self.encoder(combined_features, pos, labels)
        
        # Ø§ØµÙ„Ø§Ø­: Ø§Ù†Ú©ÙˆØ¯Ø± Ù…Ø§ Ø¯Ø± Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯ÛŒØ¯ØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø±Ø§ Ù‡Ù… Ø¯Ø± Ù„ÛŒØ³Øª Ø®Ø±ÙˆØ¬ÛŒâ€ŒØ§Ø´ Ø¯Ø§Ø±Ø¯.
        # Ù¾Ø³ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø§Ø±Ø³Ø§Ù„ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù†ÛŒØ³Øª.
        
        logits, final_labels = self.decoder(encoder_features_list, positions_list, sampled_labels_list)
        return logits, final_labels

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)