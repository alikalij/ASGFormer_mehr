# data/dataset.py

import os
import h5py
import numpy as np
import torch
from torch.utils.data import Dataset
from torch_geometric.data import Data
import random # âœ… Ø¬Ø¯ÛŒØ¯: Ø¨Ø±Ø§ÛŒ Augmentation

def read_file_list(file_path):
    """Ø®ÙˆØ§Ù†Ø¯Ù† Ù„ÛŒØ³Øª Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø§Ø² ÛŒÚ© ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ."""
    with open(file_path, 'r') as f:
        return [line.strip() for line in f if line.strip()]

def compute_class_weights(labels_list, num_classes):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ²Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„."""
    total_labels = np.concatenate(labels_list)
    class_counts = np.bincount(total_labels, minlength=num_classes)
    
    # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ± Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯
    total_samples = class_counts.sum()
    if total_samples == 0:
        return torch.ones(num_classes, dtype=torch.float32)

    class_weights = total_samples / (num_classes * class_counts + 1e-6)
    return torch.tensor(class_weights, dtype=torch.float32)

# data/dataset.py

import os
import h5py
import numpy as np
import torch
from torch.utils.data import Dataset
from torch_geometric.data import Data

# ... (ØªÙˆØ§Ø¨Ø¹ read_file_list Ùˆ compute_class_weights Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù†Ø¯) ...

class PointCloudProcessor:
    """Ú©Ù„Ø§Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨Ø± Ù†Ù‚Ø§Ø·."""
    def __init__(self, num_points, is_training=True):
        self.num_points = num_points
        self.is_training = is_training # Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Augmentation Ø¨Ø§ÛŒØ¯ Ø§Ø¹Ù…Ø§Ù„ Ø´ÙˆØ¯ ÛŒØ§ Ø®ÛŒØ±

    def _normalize_points(self, points_features):
        """
        Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
        1. Ù…Ø±Ú©Ø²ÛŒØª XYZ Ø¨Ø§ Ú©Ù… Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†.
        2. Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ XYZ Ø¨Ø±Ø§ÛŒ Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯Ø± Ú©Ø±Ù‡ ÙˆØ§Ø­Ø¯.
        3. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ù†Ú¯ RGB Ø¨Ù‡ [0, 1].
        """
        # --- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ XYZ ---
        xyz = points_features[:, :3]

        # 1. Ù…Ø±Ú©Ø²ÛŒØª (Centering)
        centroid = np.mean(xyz, axis=0)
        xyz_centered = xyz - centroid

        # 2. Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ Ú©Ø±Ù‡ ÙˆØ§Ø­Ø¯ (Scaling to Unit Sphere)
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø§Ù‚Ù„ÛŒØ¯Ø³ÛŒ Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø§Ø² Ù…Ø¨Ø¯Ø£ Ø¬Ø¯ÛŒØ¯ (0,0,0)
        distances = np.sqrt(np.sum(xyz_centered**2, axis=1))
        # ÛŒØ§ÙØªÙ† Ø­Ø¯Ø§Ú©Ø«Ø± ÙØ§ØµÙ„Ù‡ (Ø´Ø¹Ø§Ø¹)
        max_distance = np.max(distances)
        # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ± Ø§Ú¯Ø± Ù‡Ù…Ù‡ Ù†Ù‚Ø§Ø· Ø¯Ø± ÛŒÚ© Ù†Ù‚Ø·Ù‡ Ø¨Ø§Ø´Ù†Ø¯
        if max_distance > 1e-6:
             xyz_normalized = xyz_centered / max_distance
        else:
             xyz_normalized = xyz_centered # Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„Øª Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ Ù†ÛŒØ³Øª

        # --- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ RGB ---
        rgb = points_features[:, 3:6]
        rgb_normalized = rgb / 255.0

        # --- Ù†Ø±Ù…Ø§Ù„â€ŒÙ‡Ø§ (Normals) ---
        # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ù†Ø±Ù…Ø§Ù„â€ŒÙ‡Ø§ Ø¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 6, 7, 8 Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ§Ø­Ø¯ Ù‡Ø³ØªÙ†Ø¯
        normals = points_features[:, 6:9]

        # âœ… Ø¬Ø¯ÛŒØ¯: Height Appending (Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø±ØªÙØ§Ø¹ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡)
        # z Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ (Ø³ØªÙˆÙ† Ø³ÙˆÙ… xyz_normalized) Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ù‡Ù… Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        normalized_z = xyz_normalized[:, 2:3]

        # ØªØ±Ú©ÛŒØ¨ Ù…Ø¬Ø¯Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡
        normalized_features = np.hstack((xyz_normalized, rgb_normalized, normals, normalized_z))

        return normalized_features # Ø§Ú©Ù†ÙˆÙ† 10 ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯

    def _apply_augmentation(self, points_features):
        """Ø§Ø¹Ù…Ø§Ù„ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (ÙÙ‚Ø· Ø¯Ø± Ø­Ø§Ù„Øª Ø¢Ù…ÙˆØ²Ø´)."""
        
        # --- 1. Data Scaling (Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ ØªØµØ§Ø¯ÙÛŒ) ---
        scale = np.random.uniform(0.9, 1.1)
        points_features[:, :3] *= scale # ÙÙ‚Ø· XYZ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯

        # --- 2. Color Drop (Ø­Ø°Ù Ø±Ù†Ú¯ ØªØµØ§Ø¯ÙÛŒ) ---
        if np.random.rand() < 0.2: # Û²Û°Ùª Ø§Ø­ØªÙ…Ø§Ù„ Ø­Ø°Ù Ø±Ù†Ú¯
             points_features[:, 3:6] = 0.0 # Ø±Ù†Ú¯â€ŒÙ‡Ø§ ØµÙØ± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
             
        # --- 3. Jitter (Ù„Ø±Ø²Ø´ Ø¬Ø²Ø¦ÛŒ) (ÛŒÚ© ØªÚ©Ù†ÛŒÚ© Ø±Ø§ÛŒØ¬ Ø¯ÛŒÚ¯Ø±) ---
        jitter = np.random.normal(0, 0.02, (points_features.shape[0], 3))
        points_features[:, :3] += jitter

        # --- 4. Ú†Ø±Ø®Ø´ ØªØµØ§Ø¯ÙÛŒ Ø­ÙˆÙ„ Ù…Ø­ÙˆØ± Z (Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù…) ---
        angle = np.random.uniform(0, 2 * np.pi)
        cos_a, sin_a = np.cos(angle), np.sin(angle)
        rotation_matrix = np.array([
            [cos_a, -sin_a, 0],
            [sin_a,  cos_a, 0],
            [0,      0,     1]
        ])
        # Ø§Ø¹Ù…Ø§Ù„ Ú†Ø±Ø®Ø´ Ø¨Ù‡ XYZ Ùˆ Normals
        points_features[:, :3] = points_features[:, :3] @ rotation_matrix.T
        points_features[:, 6:9] = points_features[:, 6:9] @ rotation_matrix.T
        
        return points_features

    def process(self, data, labels):
        """Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§."""

        # --- Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ (Ù‚Ø¨Ù„ Ø§Ø² Augmentation) ---
        num_original_points = len(data)
        if num_original_points > self.num_points:
            choice = np.random.choice(num_original_points, self.num_points, replace=False)
        else:
            choice = np.random.choice(num_original_points, self.num_points, replace=True)
            
        sampled_data = data[choice, :]
        sampled_labels = labels[choice]

        # --- Ø§Ø¹Ù…Ø§Ù„ Augmentation (ÙÙ‚Ø· Ø¯Ø± Ø­Ø§Ù„Øª Ø¢Ù…ÙˆØ²Ø´) ---
        if self.is_training:
            # Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØºÛŒÛŒØ± Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø´ Ø´Ø¯Ù‡ (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯)
            sampled_data = self._apply_augmentation(sampled_data.copy())

        # --- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Ø±ÙˆÛŒ Ù†Ù‚Ø§Ø· Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø´Ø¯Ù‡) ---
        # âœ… ØªØºÛŒÛŒØ±: Ø§Ú©Ù†ÙˆÙ† 10 ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ (Ø¨Ø§ Ø§Ø­ØªØ³Ø§Ø¨ Ø§Ø±ØªÙØ§Ø¹)
        # ğŸ’¡ Ù†Ú©ØªÙ‡: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
        # ØªØ§ Ù…Ø±Ú©Ø²ÛŒØª Ùˆ Ù…Ù‚ÛŒØ§Ø³ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„ Ø§Ø¨Ø± Ù†Ù‚Ø§Ø· Ø§ØµÙ„ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆØ¯.
        normalized_features = self._normalize_points(sampled_data)
                
        return torch.from_numpy(normalized_features).float(), torch.from_numpy(sampled_labels).long()


class H5Dataset(Dataset):
    """Ú©Ù„Ø§Ø³ Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ H5."""
    def __init__(self, file_paths, processor, base_path):
        self.file_paths = [os.path.join(base_path, f) for f in file_paths]
        self.processor = processor

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        h5_path = self.file_paths[idx]
        with h5py.File(h5_path, 'r') as f:
            # âœ… Ø§ØµÙ„Ø§Ø­: ØªÙ…Ø§Ù… Û¹ Ø³ØªÙˆÙ† Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            data = f['data'][:]
            labels = f['label'][:].flatten()

        features, labels = self.processor.process(data, labels)
        
        # âœ… Ø§ØµÙ„Ø§Ø­: ØªÙÚ©ÛŒÚ© pos Ø§Ø² x
        # pos ÙÙ‚Ø· Ø´Ø§Ù…Ù„ XYZ Ø§Ø³Øª
        pos = features[:, :3]
        x = features # x Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù… 10 ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³Øª
        
        return Data(x=x, pos=pos, y=labels)