# data/dataset.py

import os
import h5py
import numpy as np
import torch
from torch.utils.data import Dataset
from torch_geometric.data import Data

def read_file_list(file_path):
    """Ø®ÙˆØ§Ù†Ø¯Ù† Ù„ÛŒØ³Øª Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø§Ø² ÛŒÚ© ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ."""
    with open(file_path, 'r') as f:
        return [line.strip() for line in f if line.strip()]

def compute_class_weights(labels_list, num_classes):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆØ²Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¨Ù„Ù‡ Ø¨Ø§ Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„."""
    total_labels = np.concatenate(labels_list)
    class_counts = np.bincount(total_labels, minlength=num_classes)
    
    # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ± Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯
    total_samples = class_counts.sum()
    if total_samples == 0:
        return torch.ones(num_classes, dtype=torch.float32)

    class_weights = total_samples / (num_classes * class_counts + 1e-6)
    return torch.tensor(class_weights, dtype=torch.float32)

# data/dataset.py

import os
import h5py
import numpy as np
import torch
from torch.utils.data import Dataset
from torch_geometric.data import Data

# ... (ØªÙˆØ§Ø¨Ø¹ read_file_list Ùˆ compute_class_weights Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù†Ø¯) ...

class PointCloudProcessor:
    """Ú©Ù„Ø§Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨Ø± Ù†Ù‚Ø§Ø·."""
    def __init__(self, num_points):
        self.num_points = num_points

    def _normalize_points(self, points_features):
        """
        Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
        1. Ù…Ø±Ú©Ø²ÛŒØª XYZ Ø¨Ø§ Ú©Ù… Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†.
        2. Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ XYZ Ø¨Ø±Ø§ÛŒ Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯Ø± Ú©Ø±Ù‡ ÙˆØ§Ø­Ø¯.
        3. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ù†Ú¯ RGB Ø¨Ù‡ [0, 1].
        """
        # --- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ XYZ ---
        xyz = points_features[:, :3]

        # 1. Ù…Ø±Ú©Ø²ÛŒØª (Centering)
        centroid = np.mean(xyz, axis=0)
        xyz_centered = xyz - centroid

        # 2. Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ Ú©Ø±Ù‡ ÙˆØ§Ø­Ø¯ (Scaling to Unit Sphere)
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø§Ù‚Ù„ÛŒØ¯Ø³ÛŒ Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø§Ø² Ù…Ø¨Ø¯Ø£ Ø¬Ø¯ÛŒØ¯ (0,0,0)
        distances = np.sqrt(np.sum(xyz_centered**2, axis=1))
        # ÛŒØ§ÙØªÙ† Ø­Ø¯Ø§Ú©Ø«Ø± ÙØ§ØµÙ„Ù‡ (Ø´Ø¹Ø§Ø¹)
        max_distance = np.max(distances)
        # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ± Ø§Ú¯Ø± Ù‡Ù…Ù‡ Ù†Ù‚Ø§Ø· Ø¯Ø± ÛŒÚ© Ù†Ù‚Ø·Ù‡ Ø¨Ø§Ø´Ù†Ø¯
        if max_distance > 1e-6:
             xyz_normalized = xyz_centered / max_distance
        else:
             xyz_normalized = xyz_centered # Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„Øª Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¯Ù‡ÛŒ Ù†ÛŒØ³Øª

        # --- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ RGB ---
        rgb = points_features[:, 3:6]
        rgb_normalized = rgb / 255.0

        # --- Ù†Ø±Ù…Ø§Ù„â€ŒÙ‡Ø§ (Normals) ---
        # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ù†Ø±Ù…Ø§Ù„â€ŒÙ‡Ø§ Ø¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 6, 7, 8 Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ§Ø­Ø¯ Ù‡Ø³ØªÙ†Ø¯
        normals = points_features[:, 6:9]

        # ØªØ±Ú©ÛŒØ¨ Ù…Ø¬Ø¯Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡
        normalized_features = np.hstack((xyz_normalized, rgb_normalized, normals))

        return normalized_features

    def process(self, data, labels):
        """Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§."""
        # ğŸ’¡ Ù†Ú©ØªÙ‡: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
        # ØªØ§ Ù…Ø±Ú©Ø²ÛŒØª Ùˆ Ù…Ù‚ÛŒØ§Ø³ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„ Ø§Ø¨Ø± Ù†Ù‚Ø§Ø· Ø§ØµÙ„ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆØ¯.
        normalized_features = self._normalize_points(data)

        # Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ø«Ø§Ø¨Øª
        num_original_points = len(normalized_features)
        if num_original_points > self.num_points:
            choice = np.random.choice(num_original_points, self.num_points, replace=False)
        else:
            # Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ú©Ù…ØªØ± Ø¨ÙˆØ¯ØŒ Ø¨Ø§ ØªÚ©Ø±Ø§Ø± Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø§Ù†ÛŒÙ…
            choice = np.random.choice(num_original_points, self.num_points, replace=True)

        sampled_features = normalized_features[choice, :]
        sampled_labels = labels[choice]

        return torch.from_numpy(sampled_features).float(), torch.from_numpy(sampled_labels).long()



class H5Dataset(Dataset):
    """Ú©Ù„Ø§Ø³ Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ H5."""
    def __init__(self, file_paths, processor, base_path):
        self.file_paths = [os.path.join(base_path, f) for f in file_paths]
        self.processor = processor

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        h5_path = self.file_paths[idx]
        with h5py.File(h5_path, 'r') as f:
            # âœ… Ø§ØµÙ„Ø§Ø­: ØªÙ…Ø§Ù… Û¹ Ø³ØªÙˆÙ† Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            data = f['data'][:]
            labels = f['label'][:].flatten()

        features, labels = self.processor.process(data, labels)
        
        # âœ… Ø§ØµÙ„Ø§Ø­: ØªÙÚ©ÛŒÚ© pos Ø§Ø² x
        # pos ÙÙ‚Ø· Ø´Ø§Ù…Ù„ XYZ Ø§Ø³Øª
        pos = features[:, :3]
        # x Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù… Û¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³Øª
        x = features
        
        return Data(x=x, pos=pos, y=labels)